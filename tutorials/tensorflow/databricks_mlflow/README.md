# Overview

This code implements a Logistic Regression model using the Google Cloud 
Platform.
It includes code to process data, train a TensorFlow model with 
hyperparameter tuning, run predictions on new data and
assess model performance.

The sample provided uses `tf.keras`, TensorFlow's implementation of the 
Keras API specification. This provides the benefits of Keras and also 
first-class support for TensorFlow-specific functionality.

## **Data description**

The [Census Income Data
Set](https://archive.ics.uci.edu/ml/datasets/Census+Income) that this 
sample uses for training is hosted by the [UC Irvine Machine Learning
Repository](https://archive.ics.uci.edu/ml/datasets/).

Using census data which contains data a person's age, education, marital 
status, and occupation (the features), we will try to predict whether or 
not the person earns more than 50,000 dollars a year (the target label). 
We will train a logistic regression model that, given an individual's 
information, outputs a number between 0 and 1, this can be interpreted 
as the probability that the individual has an annual income of over 
50,000 dollars.

As a modeler and developer, think about how this data is used and the 
potential benefits and harm a model's predictions can cause. A model 
like this could reinforce societal biases and disparities. Is each 
feature relevant to the problem you want to solve or will it introduce 
bias? For more information, read about [ML fairness](https://developers.google.com/machine-learning/fairness-overview/).

### **Disclaimer**

This dataset is provided by a third party. Google provides no 
representation, warranty, or other guarantees about the validity or any 
other aspects of this dataset.

### **Install dependencies**

Install the python dependencies. 

```
pip install --upgrade -r requirements.txt
```

###  **Run model locally:**

```
DATE=`date '+%Y%m%d_%H%M%S'`
export JOB_DIR=mlflow
export TRAIN_FILE=gs://cloud-samples-data/ml-engine/census/data/adult.data.csv
export EVAL_FILE=gs://cloud-samples-data/ml-engine/census/data/adult.test.csv
export TRAIN_STEPS=1000
export BUCKET_NAME=<Your GCS bucket>
export PROJECT_ID=<Your Project id>
```

#### Run using Python

```
python -m trainer.task --train-files $TRAIN_FILE \
    --eval-files $EVAL_FILE \
    --job-dir $JOB_DIR \
    --train-steps $TRAIN_STEPS \
    --eval-steps 100
```

#### Deploy model in GCP after MLflow

```
python -m trainer.task -W ignore --train-files $TRAIN_FILE \ 
    --eval-files $EVAL_FILE \
    --job-dir $JOB_DIR \
    --train-steps $TRAIN_STEPS \
    --eval-steps 1 \
    --num-epochs=20 \
    --deploy-gcp \
    --gcs-bucket=$BUCKET_NAME
    --project-id=$PROJECT_ID
```

This will generate a local mlruns folder where you can see the artifacts
generated by MLflow, from your terminal type:

```
mlflow ui
```

You will see all the runs.


#### Run locally via the `gcloud` command for AI Platform:

```
gcloud ai-platform local train --package-path trainer \
    --module-name trainer.task \
    -- \
    --train-files $TRAIN_FILE \
    --eval-files $EVAL_FILE \
    --job-dir $JOB_DIR \
    --train-steps $TRAIN_STEPS \
    --eval-steps 100
```

#### Hyperparameter tuning:

You can optionally perform hyperparameter tuning by using the included 
hptuning_config.yaml configuration file. This file tells AI Platform to 
tune the batch size and learning rate for training over multiple trials 
to maximize accuracy.

In this example, the training code uses a [TensorBoard callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard), 
which creates TensorFlow Summary Events during training. AI Platform uses 
these events to track the metric you want to optimize. 
Learn more about [hyperparameter tuning in AI Platform Training](https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview).

## References

[Tensorflow tutorial](https://www.tensorflow.org/guide/premade_estimators)